# ğŸ¤– Phi-3 Chat Interface

Interface web pour chatrer avec votre modÃ¨le Phi-3 local avec authentification et systÃ¨me de preprompts.

## âœ¨ FonctionnalitÃ©s

- ğŸ” **Authentification simple** avec login/mot de passe
- ğŸ’¬ **Interface de chat** intuitive et responsive
- ğŸ“ **Preprompts personnalisÃ©s** sauvegardÃ©s et rÃ©utilisables
- ğŸ¨ **Interface moderne** avec design gradient
- ğŸ“± **Responsive** - fonctionne sur mobile et desktop
- âš¡ **Temps rÃ©el** - chat fluide avec le modÃ¨le local

## ğŸ› ï¸ Installation

### PrÃ©requis

- Node.js (version 14 ou plus rÃ©cente)
- llama.cpp compilÃ© avec votre modÃ¨le Phi-3

### Ã‰tapes d'installation

1. **Cloner le repository**
```bash
git clone https://github.com/creach-t/phi3-chat-interface.git
cd phi3-chat-interface
```

2. **Installer les dÃ©pendances**
```bash
npm install
```

3. **Configurer les chemins dans server.js**
```javascript
const config = {
  username: 'admin',
  passwordHash: '$2b$10$8K1p/a0dF2h5j6k8l9M0n.O1P2Q3R4S5T6U7V8W9X0Y1Z2a3b4c5d6',
  llamaCppPath: './build/bin/llama-cli',  // Chemin vers votre llama-cli
  modelPath: './models/Phi-3-mini-4k-instruct-Q2_K.gguf'  // Chemin vers votre modÃ¨le
};
```

4. **DÃ©marrer le serveur**
```bash
npm start
```

5. **AccÃ©der Ã  l'interface**
Ouvrez votre navigateur sur `http://localhost:3000`

## ğŸ”‘ Authentification par dÃ©faut

- **Username:** `admin`
- **Password:** `password123`

### Changer le mot de passe

Pour gÃ©nÃ©rer un nouveau hash de mot de passe :

```bash
node -e "const bcrypt = require('bcrypt'); bcrypt.hash('votre_nouveau_mot_de_passe', 10).then(hash => console.log('Nouveau hash:', hash));"
```

Copiez le hash gÃ©nÃ©rÃ© dans `server.js` Ã  la place de `passwordHash`.

## ğŸ“ Utilisation des Preprompts

Les preprompts vous permettent de :
- DÃ©finir un contexte ou un rÃ´le pour l'IA
- CrÃ©er des templates rÃ©utilisables
- AmÃ©liorer la cohÃ©rence des rÃ©ponses

### Exemples de preprompts

**Assistant franÃ§ais :**
```
Tu es un assistant IA qui rÃ©pond toujours en franÃ§ais de maniÃ¨re polie et professionnelle.
```

**Expert en code :**
```
Tu es un expert en programmation. Fournit des rÃ©ponses techniques prÃ©cises avec des exemples de code quand appropriÃ©.
```

**Tuteur pÃ©dagogique :**
```
Tu es un tuteur patient qui explique les concepts de maniÃ¨re simple avec des exemples concrets.
```

## ğŸš€ DÃ©ploiement

### Sur un VPS

1. **TransfÃ©rer les fichiers sur votre VPS**
2. **Installer Node.js sur le VPS**
3. **Configurer PM2 pour la production** (optionnel)
```bash
npm install -g pm2
pm2 start server.js --name phi3-chat
pm2 startup
pm2 save
```

4. **Configurer un proxy nginx** (optionnel)
```nginx
server {
    listen 80;
    server_name votre-domaine.com;
    
    location / {
        proxy_pass http://localhost:3000;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_cache_bypass $http_upgrade;
    }
}
```

### Variables d'environnement

Vous pouvez utiliser des variables d'environnement :

```bash
export PORT=3000
export LLAMA_CPP_PATH="/path/to/llama-cli"
export MODEL_PATH="/path/to/model.gguf"
npm start
```

## ğŸ“ Structure du projet

```
phi3-chat-interface/
â”œâ”€â”€ package.json          # DÃ©pendances npm
â”œâ”€â”€ server.js             # Serveur Express principal
â”œâ”€â”€ preprompts.json       # Stockage des preprompts (auto-gÃ©nÃ©rÃ©)
â”œâ”€â”€ README.md             # Documentation
â””â”€â”€ public/
    â”œâ”€â”€ index.html        # Interface utilisateur
    â”œâ”€â”€ style.css         # Styles CSS
    â””â”€â”€ script.js         # JavaScript frontend
```

## ğŸ”§ Configuration avancÃ©e

### ParamÃ¨tres llama.cpp

Vous pouvez modifier les paramÃ¨tres dans `server.js` :

```javascript
const llamaProcess = spawn(config.llamaCppPath, [
  '-m', config.modelPath,
  '-p', fullPrompt,
  '-c', '2048',     // Contexte
  '-n', '512',      // Tokens max
  '--temp', '0.7',  // TempÃ©rature
  '--no-display-prompt'
]);
```

### Timeout et sÃ©curitÃ©

- Timeout par dÃ©faut : 60 secondes
- Session : 24 heures
- HTTPS recommandÃ© en production

## ğŸ› DÃ©pannage

### Le modÃ¨le ne se charge pas
- VÃ©rifiez le chemin vers `llama-cli`
- VÃ©rifiez le chemin vers le modÃ¨le `.gguf`
- Assurez-vous que llama.cpp est compilÃ© correctement

### Erreur de mÃ©moire
- Utilisez un modÃ¨le plus petit (Q2_K au lieu de Q4_K)
- RÃ©duisez la taille du contexte (`-c`)
- Fermez d'autres applications

### Interface ne se charge pas
- VÃ©rifiez que le port 3000 est libre
- Regardez les logs du serveur pour les erreurs
- Assurez-vous que les fichiers `public/` sont prÃ©sents

## ğŸ“„ Licence

MIT License - Libre d'utilisation et de modification.

## ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  :
- Signaler des bugs
- Proposer des amÃ©liorations
- Ajouter des fonctionnalitÃ©s

---

**Profitez de votre IA locale ! ğŸš€**