# ğŸ¤– Phi-3 Chat Interface

Interface web pour interagir avec votre modÃ¨le Phi-3 local avec authentification et systÃ¨me de preprompts.

## âœ¨ FonctionnalitÃ©s

- ğŸ” **Authentification sÃ©curisÃ©e** avec login/mot de passe
- ğŸ’¬ **Interface de chat** moderne et responsive  
- ğŸ“ **Preprompts personnalisÃ©s** sauvegardÃ©s et rÃ©utilisables
- âš™ï¸ **Configuration** des paramÃ¨tres du modÃ¨le
- ğŸ“Š **Logging** avec Winston
- ğŸ”§ **TypeScript** pour une meilleure maintenabilitÃ©

## ğŸš€ Installation

### PrÃ©requis
- Node.js 18+
- llama.cpp compilÃ© avec votre modÃ¨le Phi-3

### Ã‰tapes

```bash
# Cloner le repository
git clone https://github.com/creach-t/phi3-chat-interface.git
cd phi3-chat-interface

# Installer les dÃ©pendances
npm install

# Configurer l'environnement
cp .env.example .env

# Compiler et dÃ©marrer
npm run build
npm start
```

## âš™ï¸ Configuration

CrÃ©ez un fichier `.env` :

```env
PORT=3000
IP_ADDRESS=0.0.0.0
NODE_ENV=production

# Authentification (changez ces valeurs !)
AUTH_USERNAME=admin
AUTH_PASSWORD=votre_mot_de_passe

# Chemins vers llama.cpp et le modÃ¨le
LLAMA_CPP_PATH=/path/to/llama-cli
MODEL_PATH=/path/to/phi3-model.gguf

# Logging
LOG_LEVEL=info
LOG_FILE=./logs/app.log
```

## ğŸ› ï¸ Scripts

```bash
npm run dev        # DÃ©veloppement avec rechargement
npm run build      # Compiler TypeScript
npm start          # DÃ©marrer en production
npm run start:dev  # DÃ©veloppement avec nodemon
```

## ğŸ” Authentification

**Identifiants par dÃ©faut :**
- Username: `admin`
- Password: `password123`

âš ï¸ **Changez le mot de passe** dans le fichier `.env` avant de dÃ©ployer !

## ğŸ“ Preprompts

Les preprompts permettent de dÃ©finir un contexte pour l'IA. Exemples :

```
Tu es un assistant technique expert en programmation.
```

```
Tu es un tuteur patient qui explique simplement les concepts.
```

## ğŸ“ Structure

```
phi3-chat-interface/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config/        # Configuration
â”‚   â”œâ”€â”€ controllers/   # ContrÃ´leurs
â”‚   â”œâ”€â”€ services/      # Services mÃ©tier
â”‚   â”œâ”€â”€ routes/        # Routes API
â”‚   â””â”€â”€ app.ts         # App Express
â”œâ”€â”€ public/            # Interface web
â”œâ”€â”€ data/              # DonnÃ©es (logs, preprompts)
â””â”€â”€ server.ts          # Point d'entrÃ©e
```

## ğŸš€ DÃ©ploiement

### Avec PM2
```bash
npm run build
pm2 start dist/server.js --name phi3-chat
```

### Variables d'environnement importantes
- `LLAMA_CPP_PATH` : Chemin vers l'exÃ©cutable llama-cli
- `MODEL_PATH` : Chemin vers votre modÃ¨le .gguf
- `AUTH_PASSWORD` : Mot de passe sÃ©curisÃ©

## ğŸ› DÃ©pannage

**Le serveur ne dÃ©marre pas :**
- VÃ©rifiez que le port 3000 est libre
- ContrÃ´lez la configuration `.env`

**Le modÃ¨le ne rÃ©pond pas :**
- VÃ©rifiez les chemins `LLAMA_CPP_PATH` et `MODEL_PATH`
- Consultez les logs dans `data/logs/`

**Erreurs de mÃ©moire :**
- Utilisez un modÃ¨le plus petit (Q2_K au lieu de Q4_K)
- RÃ©duisez la taille du contexte

## ğŸ“„ Licence

MIT License

---

**AccÃ©dez Ã  votre interface sur http://localhost:3000 ğŸš€**