# ü§ñ Phi-3 Chat Interface

Interface web pour interagir avec votre mod√®le Phi-3 local avec authentification, syst√®me de preprompts et **gestion de mod√®les**.

## ‚ú® Fonctionnalit√©s

- üîê **Authentification s√©curis√©e** avec login/mot de passe
- üí¨ **Interface de chat** moderne et responsive  
- üìù **Preprompts personnalis√©s** sauvegard√©s et r√©utilisables
- ü§ñ **Gestion des mod√®les** - T√©l√©charger, activer et supprimer des mod√®les depuis l'interface
- ‚öôÔ∏è **Configuration** des param√®tres du mod√®le
- üìä **Logging** avec Winston
- üîß **TypeScript** pour une meilleure maintenabilit√©

## üöÄ Installation

### Pr√©requis
- Node.js 18+
- llama.cpp compil√© avec votre mod√®le Phi-3
- `curl` install√© (pour les t√©l√©chargements de mod√®les)

### √âtapes

```bash
# Cloner le repository
git clone https://github.com/creach-t/phi3-chat-interface.git
cd phi3-chat-interface

# Basculer sur la branche de gestion des mod√®les
git checkout feature/model-manager

# Installer les d√©pendances
npm install

# Configurer l'environnement
cp .env.example .env

# Compiler et d√©marrer
npm run build
npm start
```

## ‚öôÔ∏è Configuration

Cr√©ez un fichier `.env` :

```env
PORT=3000
IP_ADDRESS=0.0.0.0
NODE_ENV=production

# Authentification (changez ces valeurs !)
AUTH_USERNAME=admin
AUTH_PASSWORD=votre_mot_de_passe

# Chemins vers llama.cpp et le mod√®le
LLAMA_CPP_PATH=/path/to/llama-cli
MODEL_PATH=/path/to/phi3-model.gguf

# Logging
LOG_LEVEL=info
LOG_FILE=./logs/app.log
```

## üõ†Ô∏è Scripts

```bash
npm run dev        # D√©veloppement avec rechargement
npm run build      # Compiler TypeScript
npm start          # D√©marrer en production
npm run start:dev  # D√©veloppement avec nodemon
```

## üîê Authentification

**Identifiants par d√©faut :**
- Username: `admin`
- Password: `password123`

‚ö†Ô∏è **Changez le mot de passe** dans le fichier `.env` avant de d√©ployer !

## ü§ñ Nouvelle Fonctionnalit√© : Gestion des Mod√®les

### Vue d'ensemble
La nouvelle interface permet de :
- **T√©l√©charger** des mod√®les directement depuis Hugging Face
- **Activer** le mod√®le de votre choix
- **Supprimer** les mod√®les inutiles
- **Suivre** le progr√®s des t√©l√©chargements en temps r√©el

### Utilisation

#### 1. T√©l√©charger un mod√®le
- Coller l'URL Hugging Face d'un mod√®le GGUF
- Exemple : `https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf`
- Optionnel : sp√©cifier un nom de fichier personnalis√©
- Cliquer sur "T√©l√©charger"

#### 2. Activer un mod√®le
- Dans la liste des mod√®les disponibles
- Cliquer sur "Activer" pour le mod√®le souhait√©
- Le mod√®le actuel est indiqu√© par üü¢

#### 3. Supprimer un mod√®le
- Cliquer sur "Supprimer" (impossible pour le mod√®le actif)
- Confirmer la suppression

### Mod√®les Recommand√©s

**Phi-3 Mini (Recommand√© pour d√©buter) :**
```
https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
```

**Phi-3 Mini (Version l√©g√®re) :**
```
https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q2_k.gguf
```

**Phi-3 Medium (Plus performant, plus lourd) :**
```
https://huggingface.co/microsoft/Phi-3-medium-4k-instruct-gguf/resolve/main/Phi-3-medium-4k-instruct-q4.gguf
```

### Tailles des Mod√®les
- **Q2_K** : ~2 GB (plus petit, moins pr√©cis)
- **Q4_K** : ~2.4 GB (√©quilibre qualit√©/taille)
- **Q8_K** : ~4.5 GB (plus pr√©cis, plus volumineux)

## üìù Preprompts

Les preprompts permettent de d√©finir un contexte pour l'IA. Exemples :

```
Tu es un assistant technique expert en programmation.
```

```
Tu es un tuteur patient qui explique simplement les concepts.
```

## üìÅ Structure

```
phi3-chat-interface/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/        # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ controllers/   # Contr√¥leurs (+ modelsController)
‚îÇ   ‚îú‚îÄ‚îÄ services/      # Services m√©tier (+ modelManager)
‚îÇ   ‚îú‚îÄ‚îÄ routes/        # Routes API (+ models)
‚îÇ   ‚îî‚îÄ‚îÄ app.ts         # App Express
‚îú‚îÄ‚îÄ public/            # Interface web
‚îÇ   ‚îú‚îÄ‚îÄ css/           # Styles (+ models.css)
‚îÇ   ‚îî‚îÄ‚îÄ js/            # Scripts (+ models.js, main.js)
‚îú‚îÄ‚îÄ models/            # Dossier des mod√®les GGUF
‚îú‚îÄ‚îÄ data/              # Donn√©es (logs, preprompts)
‚îî‚îÄ‚îÄ server.ts          # Point d'entr√©e
```

## üöÄ D√©ploiement

### Avec PM2
```bash
npm run build
pm2 start dist/server.js --name phi3-chat
```

### Variables d'environnement importantes
- `LLAMA_CPP_PATH` : Chemin vers l'ex√©cutable llama-cli
- `MODEL_PATH` : Chemin vers votre mod√®le .gguf (sera mis √† jour automatiquement)
- `AUTH_PASSWORD` : Mot de passe s√©curis√©

## üêõ D√©pannage

**Le serveur ne d√©marre pas :**
- V√©rifiez que le port 3000 est libre
- Contr√¥lez la configuration `.env`

**Le mod√®le ne r√©pond pas :**
- V√©rifiez les chemins `LLAMA_CPP_PATH` et `MODEL_PATH`
- Consultez les logs dans `data/logs/`

**T√©l√©chargement de mod√®le √©choue :**
- V√©rifiez que `curl` est install√©
- V√©rifiez l'URL Hugging Face (doit pointer vers un fichier .gguf)
- V√©rifiez l'espace disque disponible

**Erreurs de m√©moire :**
- Utilisez un mod√®le plus petit (Q2_K au lieu de Q4_K)
- R√©duisez la taille du contexte

## üîÑ API Endpoints pour les Mod√®les

```bash
GET    /api/models                           # Liste des mod√®les
POST   /api/models/download                  # T√©l√©charger un mod√®le
POST   /api/models/activate                  # Activer un mod√®le
DELETE /api/models/:filename                 # Supprimer un mod√®le
GET    /api/models/downloads                 # Status des t√©l√©chargements
POST   /api/models/downloads/:id/cancel      # Annuler un t√©l√©chargement
```

## üìÑ Licence

MIT License

---

**Acc√©dez √† votre interface sur http://localhost:3000 üöÄ**

> Nouvelle fonctionnalit√© de gestion des mod√®les d√©velopp√©e pour simplifier l'utilisation de Phi-3 !